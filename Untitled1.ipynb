{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3491bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf08a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f533c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Backtesting.ipynb\t\t        NIFTY_best.keras\r\n",
      " beat_target_scaler.pkl\t\t        NIFTY.keras\r\n",
      " best_feature_scaler.pkl\t        NiftyPrediction.png\r\n",
      "'Binary Classification'\t\t        Notes\r\n",
      " Datasets\t\t\t        README.md\r\n",
      " feature_scaler.pkl\t\t       'Regression datasets'\r\n",
      " Financial_Data.ipynb\t\t        Regression.ipynb\r\n",
      "'Making the Datasets'\t\t        target_scaler.pkl\r\n",
      "'Making the regression dataset.ipynb'   Untitled1.ipynb\r\n",
      " NIFTY50_3y.csv\t\t\t       'Web Scraping'\r\n",
      " NIFTY50.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4329ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('NIFTY_best.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608217af",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler = joblib.load('best_feature_scaler.pkl')\n",
    "target_scaler = joblib.load('beat_target_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a1f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty = yf.Ticker('^NSEI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6df93cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nifty.history(start=\"2024-10-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f5da625",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6110e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a00b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(name, qty, data=data):\n",
    "    for i in range(1, qty+1):\n",
    "        data[name+str(i)] = data[name].shift(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c53fc19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n",
      "/tmp/ipykernel_8960/2251644222.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name+str(i)] = data[name].shift(i)\n"
     ]
    }
   ],
   "source": [
    "make_features('Close', 25, data)\n",
    "make_features('Open', 25, data)\n",
    "make_features('High', 25,data)\n",
    "make_features('Low', 25,data)\n",
    "make_features('Volume', 25, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66cd57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Predict\"] = data[\"Close\"].shift(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a24a678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 134)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e40af2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends',\n",
       "       'Stock Splits', 'Predict', 'Close1',\n",
       "       ...\n",
       "       'Volume16', 'Volume17', 'Volume18', 'Volume19', 'Volume20', 'Volume21',\n",
       "       'Volume22', 'Volume23', 'Volume24', 'Volume25'],\n",
       "      dtype='object', length=134)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b24de09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_scaler.transform(data.drop(columns=['Date', 'Predict', 'Dividends','Stock Splits']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24e52589",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X, verbose=0)\n",
    "prediction = target_scaler.inverse_transform(pred.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e000473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [      nan],\n",
       "       [23888.852],\n",
       "       [23547.906],\n",
       "       [24267.877],\n",
       "       [23553.012]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc5e3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(new_data, feature_scaler=feature_scaler1,\n",
    "                   target_scaler=target_scaler1, model=model1):\n",
    "    \"\"\"\n",
    "    Make predictions using the best model and scalers\n",
    "    \"\"\"\n",
    "    # Transform features\n",
    "    features = new_data[['Open', 'High', 'Low', 'Close', 'Volume','returns_1d', 'returns_3d','returns_5d', 'true_range', 'atr_5', 'volume_ma5', 'volume_ratio','gap', 'gap_percentage']].values\n",
    "    dates = new_data[['Day', 'Month', 'Year','day_of_week', 'day_of_month']].values\n",
    "\n",
    "    features_scaled = feature_scaler.transform(features)\n",
    "\n",
    "    # Create sequence\n",
    "    X = []\n",
    "    if len(features_scaled) >= 25:  # Make sure we have enough data\n",
    "        norm_sequence = features_scaled[-25:]\n",
    "        date_sequence = dates[-25:]\n",
    "        full_sequence = np.concatenate([norm_sequence, date_sequence], axis=1)\n",
    "        X.append(full_sequence)\n",
    "\n",
    "    X = np.array(X)\n",
    "\n",
    "    # Make prediction\n",
    "    pred_scaled = model.predict(X, verbose=0)\n",
    "\n",
    "    # Inverse transform prediction\n",
    "    prediction = target_scaler.inverse_transform(pred_scaled.reshape(-1, 1))\n",
    "\n",
    "    return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ced2ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = make_prediction(data, feature_scaler, target_scaler, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "272eaf72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac97e277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22591.555, 20971.922)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13429131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix 0: shape (19,)\n",
      "Weight matrix 1: shape (19,)\n",
      "Weight matrix 2: shape (19,)\n",
      "Weight matrix 3: shape (19,)\n",
      "Weight matrix 4: shape (19, 512)\n",
      "Weight matrix 5: shape (128, 512)\n",
      "Weight matrix 6: shape (512,)\n",
      "Weight matrix 7: shape (128,)\n",
      "Weight matrix 8: shape (128,)\n",
      "Weight matrix 9: shape (128,)\n",
      "Weight matrix 10: shape (128,)\n",
      "Weight matrix 11: shape (128, 256)\n",
      "Weight matrix 12: shape (64, 256)\n",
      "Weight matrix 13: shape (256,)\n",
      "Weight matrix 14: shape (64,)\n",
      "Weight matrix 15: shape (64,)\n",
      "Weight matrix 16: shape (64,)\n",
      "Weight matrix 17: shape (64,)\n",
      "Weight matrix 18: shape (64, 128)\n",
      "Weight matrix 19: shape (32, 128)\n",
      "Weight matrix 20: shape (128,)\n",
      "Weight matrix 21: shape (32, 1)\n",
      "Weight matrix 22: shape (1,)\n"
     ]
    }
   ],
   "source": [
    "# Get all weights\n",
    "weights = model1.get_weights()\n",
    "\n",
    "# Display weights\n",
    "for i, weight in enumerate(weights):\n",
    "    print(f\"Weight matrix {i}: shape {weight.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62db31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_weights('best_model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2836e290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'adam',\n",
       " 'learning_rate': 6.25000029685907e-05,\n",
       " 'weight_decay': None,\n",
       " 'clipnorm': 1.0,\n",
       " 'global_clipnorm': None,\n",
       " 'clipvalue': None,\n",
       " 'use_ema': False,\n",
       " 'ema_momentum': 0.99,\n",
       " 'ema_overwrite_frequency': None,\n",
       " 'loss_scale_factor': None,\n",
       " 'gradient_accumulation_steps': None,\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'epsilon': 1e-07,\n",
       " 'amsgrad': False}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87399d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
